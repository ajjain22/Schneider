{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained total of 7 models i.e 2 Random forest classifier, 2 Xgboost classifier , \n",
    "2 shallow Neural network(one each on training with and without validation set)\n",
    "and 1 deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrutijain/Applications/Python/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble.forest import RandomForestClassifier \n",
    " \n",
    "import numpy as np\n",
    "\n",
    "import re \n",
    "\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report,f1_score,accuracy_score\n",
    "\n",
    "import bisect\n",
    "\n",
    "import copy\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import hyperopt\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from hyperopt import space_eval\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report,f1_score,accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='/Users/shrutijain/Downloads/Data Scientist _SE_assignment/train/'\n",
    "all_classes = os.walk(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path='/Users/shrutijain/Downloads/Data Scientist _SE_assignment/test/'\n",
    "all_classes_test = os.walk(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_text(file_path):\n",
    "    '''\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text.\n",
    "\n",
    "    '''\n",
    "    text=''\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_df(folder_path):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text.\n",
    "\n",
    "    '''\n",
    "    all_classes = os.walk(folder_path)\n",
    "    \n",
    "    data_dict={}\n",
    "    \n",
    "    for class_ in all_classes:\n",
    "        \n",
    "        root_folder = os.path.split(class_[0])[-1]\n",
    "        \n",
    "        data_dict[root_folder]={}\n",
    "        \n",
    "        for file in class_[2]:\n",
    "            \n",
    "            \n",
    "            if (file.isdigit):\n",
    "                \n",
    "                data_dict[root_folder][file]=open(os.path.join(class_[0], file),'rb').read()\n",
    "        \n",
    "    reform = {(outerKey, innerKey): [values] for outerKey, innerDict in data_dict.items() for innerKey, values in innerDict.items()}\n",
    "    \n",
    "    df=pd.DataFrame.from_dict(reform,orient='index')\n",
    "    \n",
    "    df[0]=df[0].str.decode(\"utf-8\",errors='ignore')\n",
    "    \n",
    "    df.index = pd.MultiIndex.from_tuples(df.index)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    df.columns=['classes','file','text']\n",
    "    \n",
    "    df=df[df.groupby('classes').classes.transform('count')>1].copy().reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def data_processing(df):\n",
    "    \n",
    "    all_text=[]\n",
    "    \n",
    "    for x in df.text:\n",
    "        \n",
    "        attribute={}\n",
    "        \n",
    "        \n",
    "        \n",
    "        res_list = x.rstrip().split('\\n')\n",
    "        \n",
    "        text=[]\n",
    "        \n",
    "        for y in res_list:\n",
    "           \n",
    "            \n",
    "            coln_index=y.find(':')\n",
    "            \n",
    "            if coln_index>0:\n",
    "                \n",
    "                label=y[0:coln_index]\n",
    "                \n",
    "                value=y[coln_index+1:]\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    if (value.strip()=='') or (label.strip()==''):\n",
    "                        \n",
    "                        raise CustomError()\n",
    "                    \n",
    "                    attribute[label.lower().strip()]=value.lower().strip()\n",
    "                    \n",
    "                    \n",
    "                except CustomError:\n",
    "                    \n",
    "                    text.append(y)\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "            else:\n",
    "                text.append(y)\n",
    "                \n",
    "        other_text='\\n'.join(text)\n",
    "        \n",
    "        attribute['other_text']=other_text\n",
    "        \n",
    "        abc=copy.deepcopy(attribute)\n",
    "        \n",
    "        all_text.append(abc)\n",
    "        \n",
    "    return all_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unintented_labels(lst_dct):\n",
    "    \n",
    "    key_labels = [j for i in lst_dct for j in i.keys()]\n",
    "\n",
    "    dct = defaultdict(int)\n",
    "    \n",
    "    for key in key_labels:\n",
    "      dct[key] += 1\n",
    "    \n",
    "    #sorted=dct.reverse()\n",
    "        \n",
    "    sorted_dct=dict(sorted(dct.items(),reverse=True, key=lambda item: item[1]))\n",
    "    \n",
    "    dict_select_keys = {key:val for key, val in sorted_dct.items() if val >= len(lst_dct)*0.1}\n",
    "    \n",
    "    all_text_copy=[]\n",
    "                            \n",
    "    for row in lst_dct:\n",
    "        \n",
    "        row1 = {sel_key: row[sel_key] for sel_key in (set(row.keys()).intersection(set(dict_select_keys.keys()))) }\n",
    "        \n",
    "        other_text_lst=['{}:{}'.format(un_sel,row[un_sel]) for un_sel in set(row.keys() -set(dict_select_keys.keys()))]\n",
    "        \n",
    "        row1['other_text']=row1['other_text']+'\\n'.join(other_text_lst)\n",
    "        \n",
    "        all_text_copy.append(row1.copy())\n",
    "        \n",
    "    return all_text_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(path_):\n",
    "    \n",
    "    df = get_folder_df(path_)    \n",
    "    \n",
    "    text_attribute = data_processing(df)\n",
    "    \n",
    "    text_attribute=remove_unintented_labels(text_attribute)\n",
    "    \n",
    "    final_df = pd.DataFrame(text_attribute) \n",
    "\n",
    "    #final_df = final_df.fillna('')\n",
    "\n",
    "    final_df[\"Class\"] = \"\"\n",
    "\n",
    "    final_df['Class']=df['classes']\n",
    "    \n",
    "    final_df['all_text']=df['text']\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_line_lenght(actual_df):\n",
    "    \n",
    "    actual_df.lines = actual_df['lines'].replace('',pd.np.nan)\n",
    "    \n",
    "    actual_df['len']=actual_df.all_text.apply(lambda x:len(str(x)))\n",
    "    \n",
    "    no_nans=actual_df.dropna(subset=['lines'])\n",
    "    \n",
    "    no_nans=no_nans[no_nans.lines.str.isdigit()]\n",
    "    \n",
    "    no_nans.lines=no_nans.lines.astype(int)\n",
    "    \n",
    "    len_to_line=sum(no_nans.len)/sum(no_nans.lines)\n",
    "    \n",
    "    actual_df.lines=np.where(actual_df.lines.isna(),(actual_df.len/len_to_line).astype(int),actual_df.lines)\n",
    "    \n",
    "    actual_df.lines=np.where(~actual_df.lines.astype(str).str.isdigit(),(actual_df.len/len_to_line).astype(int),actual_df.lines)\n",
    "    \n",
    "    actual_df.lines=actual_df.lines.astype(int)\n",
    "    \n",
    "    return actual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df,column_name):\n",
    "    \n",
    "    vectorizer=TfidfVectorizer(stop_words=['<p>','</p>'],max_df=0.2,min_df=20)\n",
    "\n",
    "    tf_train=vectorizer.fit_transform(df[column_name])\n",
    "\n",
    "    tfidf_=pd.DataFrame(tf_train.toarray(),columns=[column_name+'_'+ x for x in vectorizer.get_feature_names()])\n",
    "    \n",
    "    return tfidf_,vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrutijain/Applications/Python/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "train_df=create_df(path_=train_path)\n",
    "\n",
    "train_df=interpolate_line_lenght(train_df)\n",
    "\n",
    "train_df.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_df[['nntp-posting-host', 'other_text', 'from', 'subject', 'organization',\n",
    "       'lines','reply-to','distribution','all_text']]    \n",
    "\n",
    "y=train_df['Class']\n",
    "\n",
    "    \n",
    "X_train, X_val,y_train, y_val=train_test_split (X,y, random_state=0,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "first_tfidf1,v1=tfidf(X_train,'nntp-posting-host')\n",
    "\n",
    "sec_tfidf,v2=tfidf(X_train,'other_text')\n",
    "\n",
    "third_tfidf,v3=tfidf(X_train,'from')\n",
    "\n",
    "fourth_tfidf,v4=tfidf(X_train,'subject')\n",
    "\n",
    "fifth_tfidf,v5=tfidf(X_train,'organization')\n",
    "\n",
    "sixth_tfidf,v6=tfidf(X_train,'reply-to')\n",
    "\n",
    "seventh_tfidf,v7=tfidf(X_train,'distribution')\n",
    "\n",
    "train_features=pd.concat([ first_tfidf1,sec_tfidf,third_tfidf,fourth_tfidf,fifth_tfidf,sixth_tfidf,seventh_tfidf,X_train[['lines']].reset_index(drop=True)],axis=1)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(train_features,y_train)\n",
    "\n",
    "imp_feats=pd.Series(model.feature_importances_,index=train_features.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features_xgb=imp_feats[imp_feats>0.003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other_text_dod          0.012516\n",
       "other_text_gun          0.011503\n",
       "other_text_car          0.010960\n",
       "other_text_space        0.009845\n",
       "subject_sale            0.009407\n",
       "                          ...   \n",
       "other_text_via          0.003235\n",
       "other_text_msg          0.003231\n",
       "other_text_troops       0.003153\n",
       "other_text_wondering    0.003037\n",
       "other_text_church       0.003006\n",
       "Length: 64, dtype: float32"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_features_xgb.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(n_estimators=10000,n_jobs=-1)\n",
    "\n",
    "clf.fit(train_features,y_train) \n",
    "\n",
    "\n",
    "imp_feats_rf=pd.Series(model.feature_importances_,index=train_features.columns)\n",
    "\n",
    "imp_features_rf=imp_feats_rf[imp_feats_rf>0.003]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_tfidf(df,column,vectorizer):\n",
    "    \n",
    "    tf_val=vectorizer.transform(df[column]) \n",
    "    \n",
    "    tfidf_val=pd.DataFrame(tf_val.toarray(),columns=[column+'_'+ x for x in vectorizer.get_feature_names()]) \n",
    "    \n",
    "    return tfidf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_xg(params):\n",
    "    \n",
    "    X=params['X']\n",
    "    y=params['y']\n",
    "    \n",
    "    del params['X']\n",
    "    del params['y']\n",
    "    \n",
    "    F1_Score=0\n",
    "    #print(params)\n",
    "    \n",
    "    X_train, X_test,y_train, y_test=train_test_split (X,y, random_state=2)\n",
    "\n",
    "    clf=xgb.XGBClassifier(**params)\n",
    "\n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    clf.fit(X_train, y_train,)\n",
    "\n",
    "    #print('done')\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    F1_Score = f1_score(pred,y_test,average='macro')\n",
    "    print (\"SCORE:\", F1_Score)\n",
    "    #change the metric if you like\n",
    "    return {'loss': -F1_Score/4, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_rf(params):\n",
    "    \n",
    "    X=params['X']\n",
    "    y=params['y']\n",
    "    \n",
    "    del params['X']\n",
    "    del params['y']\n",
    "    \n",
    "    F1_Score=0\n",
    "    \n",
    "    for i in range(4):\n",
    "    \n",
    "        X_train, X_test,y_train, y_test=train_test_split (X,y, random_state=i*2)\n",
    "    \n",
    "        \n",
    "        clf=RandomForestClassifier(**params)\n",
    "        \n",
    "    \n",
    "        evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    \n",
    "    \n",
    "        clf.fit(X_train, y_train,)\n",
    "\n",
    "        pred = clf.predict(X_test)\n",
    "        \n",
    "        F1_Score += f1_score(pred,y_test,average='macro')\n",
    "    print (\"SCORE:\", F1_Score)\n",
    "    #change the metric if you like\n",
    "    return {'loss': -F1_Score, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_params_xg(X_train,y_train):\n",
    "    \n",
    "    space={'max_depth':  hp.choice('max_depth', np.arange(3, 18, dtype=int)),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': hp.quniform(\"n_estimators\",50,1000,10),\n",
    "        'learning_rate':hp.quniform('learning_rate',0.02,0.6,0.001),\n",
    "        'X':X_train,\n",
    "        'y':y_train\n",
    "    }\n",
    "\n",
    "\n",
    "    trials = Trials()\n",
    "    best_param_xg = fmin(objective_function_xg,space=space, algo=tpe.suggest,\n",
    "                      max_evals=70, trials=trials)\n",
    "    \n",
    "    \n",
    "    return space_eval(space, best_param_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_params_rf(X_train,y_train):\n",
    "    \n",
    "    space={'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "           'max_features': hp.choice('max_features', range(1,3)),\n",
    "           'n_estimators': hp.choice('n_estimators', range(10,50)), \n",
    "           'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "            'X':X_train,\n",
    "            'y':y_train\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    trials = Trials()\n",
    "    best_param_rf = fmin(objective_function_rf,space=space, algo=tpe.suggest,\n",
    "                      max_evals=50, trials=trials)\n",
    "    \n",
    "    \n",
    "    return space_eval(space, best_param_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tfidf_t=val_tfidf(X_val,'nntp-posting-host',v1)\n",
    "\n",
    "sec_tfidf_t=val_tfidf(X_val,'other_text',v2)\n",
    "\n",
    "third_tfidf_t=val_tfidf(X_val,'from',v3)\n",
    "\n",
    "fourth_tfidf_t=val_tfidf(X_val,'subject',v4)\n",
    "\n",
    "fifth_tfidf_t=val_tfidf(X_val,'organization',v5)\n",
    "\n",
    "sixth_tfidf_t=val_tfidf(X_val,'reply-to',v6)\n",
    "\n",
    "seventh_tfidf_t=val_tfidf(X_val,'distribution',v7)  \n",
    "  \n",
    "test_features=pd.concat([first_tfidf_t,sec_tfidf_t,third_tfidf_t,fourth_tfidf_t,fifth_tfidf_t,sixth_tfidf_t,seventh_tfidf_t,X_val[['lines']].reset_index(drop=True)],axis=1)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:                                                \n",
      "0.00441988950276243                                   \n",
      "SCORE:                                                                                \n",
      "0.0376197953327197                                                                    \n",
      "SCORE:                                                                                \n",
      "0.00441988950276243                                                                \n",
      "SCORE:                                                                             \n",
      "0.00441988950276243                                                                \n",
      "SCORE:                                                                             \n",
      "0.00441988950276243                                                                \n",
      "SCORE:                                                                             \n",
      "0.00441988950276243                                                                \n",
      "SCORE:                                                                             \n",
      "0.00441988950276243                                                                \n",
      "SCORE:                                                                             \n",
      "0.00441988950276243                                                                \n",
      "SCORE:                                                                             \n",
      "0.00441988950276243                                                                \n",
      "SCORE:                                                                             \n",
      "0.07681913535955481                                                                \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.07681913535955481                                                                 \n",
      "SCORE:                                                                              \n",
      "0.07681913535955481                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.07681913535955481                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.07681913535955481                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.07681913535955481                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.0376197953327197                                                                  \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.07681913535955481                                                                 \n",
      "SCORE:                                                                              \n",
      "0.07681913535955481                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "SCORE:                                                                              \n",
      "0.00441988950276243                                                                 \n",
      "100%|██████████| 70/70 [40:54<00:00, 35.07s/trial, best loss: -0.019204783839888703]\n"
     ]
    }
   ],
   "source": [
    "best_params_xg=tune_params_xg(train_features[imp_features_xgb.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params_rf=tune_params_rf(train_features[imp_features_rf.index],y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####xg_boost\n",
    "\n",
    "best_model_xgb=xgb.XGBClassifier(params=best_params_xg)\n",
    "\n",
    "best_model_xgb.fit(train_features[imp_features_xgb.index],y_train)\n",
    "\n",
    "val_pred_xgb=best_model_xgb.predict(test_features[imp_features_xgb.index])\n",
    "\n",
    "fval_xgb=f1_score(y_val,val_pred_xgb,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf=best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'criterion', 'max_depth', 'max_features', 'n_estimators', 'y'])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_rf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "del best_params_rf['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "del best_params_rf['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "####rf\n",
    "\n",
    "best_model_rf=RandomForestClassifier(**best_params_rf)\n",
    "\n",
    "best_model_rf.fit(train_features[imp_features.index],y_train)\n",
    "\n",
    "val_pred_rf=best_model.predict(test_features[imp_features.index])\n",
    "\n",
    "fval_rf = f1_score(y_val,val_pred_rf,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accuracy* - F1  score weighted Average for Random forest is 48.46% for validation set(taken 20% of train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4846075326969974"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fval_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy - F1 score weighted Average for Xgboost is 63.93% for validation set(taken 20% of train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6393357322593395"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fval_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrutijain/Applications/Python/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#####TRAIN ON WHOLE DATASET\n",
    "\n",
    "w_train_df=create_df(path_=train_path)\n",
    "\n",
    "w_train_df=interpolate_line_lenght(w_train_df)\n",
    "\n",
    "w_train_df.fillna('',inplace=True)\n",
    "\n",
    "\n",
    "X=train_df[['nntp-posting-host', 'other_text', 'from', 'subject', 'organization',\n",
    "       'lines','reply-to','distribution','all_text']]    \n",
    "\n",
    "y=train_df['Class']\n",
    "\n",
    "    \n",
    "#X_train, X_val,y_train, y_val=train_test_split (X,y, random_state=0)\n",
    "\n",
    "first_tfidf1,v1=tfidf(X,'nntp-posting-host')\n",
    "\n",
    "sec_tfidf,v2=tfidf(X,'other_text')\n",
    "\n",
    "third_tfidf,v3=tfidf(X,'from')\n",
    "\n",
    "fourth_tfidf,v4=tfidf(X,'subject')\n",
    "\n",
    "fifth_tfidf,v5=tfidf(X,'organization')\n",
    "\n",
    "sixth_tfidf,v6=tfidf(X,'reply-to')\n",
    "\n",
    "seventh_tfidf,v7=tfidf(X,'distribution')\n",
    "\n",
    "train_features_w=pd.concat([ first_tfidf1,sec_tfidf,third_tfidf,fourth_tfidf,fifth_tfidf,sixth_tfidf,seventh_tfidf,X[['lines']].reset_index(drop=True)],axis=1)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrutijain/Applications/Python/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_df=create_df(path_=test_path)\n",
    "\n",
    "test_df=interpolate_line_lenght(test_df)\n",
    "    \n",
    "test_df.fillna('',inplace=True)\n",
    "\n",
    "ytest=test_df['Class']\n",
    "\n",
    "XTest=test_df[['nntp-posting-host', 'other_text', 'from', 'subject', 'organization',\n",
    "       'lines','reply-to','distribution','all_text']] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tfidf_t=val_tfidf(XTest,'nntp-posting-host',v1)\n",
    "\n",
    "sec_tfidf_t=val_tfidf(XTest,'other_text',v2)\n",
    "\n",
    "third_tfidf_t=val_tfidf(XTest,'from',v3)\n",
    "\n",
    "fourth_tfidf_t=val_tfidf(XTest,'subject',v4)\n",
    "\n",
    "fifth_tfidf_t=val_tfidf(XTest,'organization',v5)\n",
    "\n",
    "sixth_tfidf_t=val_tfidf(XTest,'reply-to',v6)\n",
    "\n",
    "seventh_tfidf_t=val_tfidf(XTest,'distribution',v7)  \n",
    "  \n",
    "test_features_w=pd.concat([first_tfidf_t,sec_tfidf_t,third_tfidf_t,fourth_tfidf_t,fifth_tfidf_t,sixth_tfidf_t,seventh_tfidf_t,XTest[['lines']].reset_index(drop=True)],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_xgb=xgb.XGBClassifier(params=best_params_xg)\n",
    "\n",
    "best_model_xgb.fit(train_features_w[imp_features.index],y)\n",
    "\n",
    "test_pred=best_model.predict(test_features_w[imp_features.index])\n",
    "\n",
    "ftest_xgb = f1_score(ytest,test_pred,average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_rf=RandomForestClassifier(**best_params)\n",
    "\n",
    "best_model_rf.fit(train_features_w[imp_features.index],y)\n",
    "\n",
    "test_pred=best_model.predict(test_features_w[imp_features.index])\n",
    "\n",
    "ftest_rf = f1_score(ytest,test_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy - F1 score weighted Average for Random forest is 38.72% for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3872944983833138"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftest_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy - F1 score weighted Average for Xgboost is 59.74% for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5974575780479489"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftest_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Nueral Network\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,f1_score,recall_score,accuracy_score,precision_score\n",
    "\n",
    "#from Preprocessing import create_sequences,create_embedding_matrix\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,LambdaCallback  \n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stemmer=SnowballStemmer('english')\n",
    "\n",
    "from keras import losses\n",
    "\n",
    "loss_fn = losses.SparseCategoricalCrossentropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train_enc=tf.keras.utils.to_categorical(le.fit_transform(y_train.values))\n",
    "\n",
    "y_test_enc=tf.keras.utils.to_categorical(le.transform(y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(len(train_features.columns),1))\n",
    "drop20 = layers.SpatialDropout1D(0.3)(input)\n",
    "conv2 = layers.Conv1D(filters=128, kernel_size=5, activation='relu')(drop20)\n",
    "drop21 = layers.Dropout(0.5)(conv2)\n",
    "conv22 = layers.Conv1D(filters=64, kernel_size=5, activation='relu')(drop21)\n",
    "drop22 = layers.Dropout(0.5)(conv22)\n",
    "pool2 = layers.MaxPooling1D(pool_size=2)(drop22)\n",
    "flat2 = layers.Flatten()(pool2)\n",
    "drop22 = layers.Dropout(0.3)(flat2)\n",
    "out = layers.Dense(20, activation='softmax')(drop22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 2295, 1)]         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 2295, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 2291, 128)         768       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2291, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 2287, 64)          41024     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2287, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1143, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 73152)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 73152)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                1463060   \n",
      "=================================================================\n",
      "Total params: 1,504,852\n",
      "Trainable params: 1,504,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_nw1 = Model(input, out)\n",
    "model_nw1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_nw1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [EarlyStopping(monitor='val_loss', patience=10,min_delta=0.0005),\\\n",
    "                  ModelCheckpoint(filepath='best_model.hdf5', monitor='val_loss',\\\n",
    "                                  save_best_only=True, mode='min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "33/33 [==============================] - 40s 1s/step - loss: 2.8279 - accuracy: 0.1643 - val_loss: 2.3191 - val_accuracy: 0.4306\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 43s 1s/step - loss: 1.7347 - accuracy: 0.4711 - val_loss: 1.6374 - val_accuracy: 0.5506\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 42s 1s/step - loss: 1.2868 - accuracy: 0.6166 - val_loss: 1.4589 - val_accuracy: 0.5694\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 32s 972ms/step - loss: 1.1144 - accuracy: 0.6633 - val_loss: 1.4059 - val_accuracy: 0.5723\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 31s 932ms/step - loss: 1.0031 - accuracy: 0.6932 - val_loss: 1.4242 - val_accuracy: 0.5910\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 32s 962ms/step - loss: 0.9529 - accuracy: 0.7091 - val_loss: 1.4447 - val_accuracy: 0.6040\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 35s 1s/step - loss: 0.9389 - accuracy: 0.7042 - val_loss: 1.5161 - val_accuracy: 0.5954\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 54s 2s/step - loss: 0.8937 - accuracy: 0.7230 - val_loss: 1.5349 - val_accuracy: 0.5896\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 41s 1s/step - loss: 0.9052 - accuracy: 0.7197 - val_loss: 1.5619 - val_accuracy: 0.6026\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 42s 1s/step - loss: 0.9264 - accuracy: 0.7100 - val_loss: 1.6267 - val_accuracy: 0.5968\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 39s 1s/step - loss: 0.9017 - accuracy: 0.7187 - val_loss: 1.6710 - val_accuracy: 0.5939\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 36s 1s/step - loss: 0.9108 - accuracy: 0.7115 - val_loss: 1.6959 - val_accuracy: 0.5939\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 37s 1s/step - loss: 0.9573 - accuracy: 0.6994 - val_loss: 1.7167 - val_accuracy: 0.5939\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.9319 - accuracy: 0.7033 - val_loss: 1.7251 - val_accuracy: 0.6069\n"
     ]
    }
   ],
   "source": [
    "history = model_nw1.fit(train_features.values,y_train_enc,\n",
    "                epochs=100,\n",
    "                verbose=1,\n",
    "                validation_data=(test_features.values, y_test_enc),\n",
    "                batch_size=64,\n",
    "                \n",
    "                #class_weight={0:1,1:10},\n",
    "                callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nw1=le.inverse_transform(np.argmax(model_nw1.predict(test_features),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy - F1 score weighted Average is 61.01% for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6101891524323161"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val,y_pred_nw1,average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_10</th>\n",
       "      <th>class_11</th>\n",
       "      <th>class_12</th>\n",
       "      <th>class_13</th>\n",
       "      <th>class_14</th>\n",
       "      <th>class_15</th>\n",
       "      <th>class_16</th>\n",
       "      <th>class_17</th>\n",
       "      <th>class_18</th>\n",
       "      <th>class_19</th>\n",
       "      <th>class_2</th>\n",
       "      <th>class_20</th>\n",
       "      <th>class_3</th>\n",
       "      <th>class_4</th>\n",
       "      <th>class_5</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_7</th>\n",
       "      <th>class_8</th>\n",
       "      <th>class_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_1</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_10</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_15</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_20</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class_1  class_10  class_11  class_12  class_13  class_14  class_15  \\\n",
       "class_1        19         0         0         0         0         1         0   \n",
       "class_10        0        23         7         0         0         1         0   \n",
       "class_11        1         2        33         1         0         0         1   \n",
       "class_12        0         0         0        26         1         0         1   \n",
       "class_13        1         0         0         3        17         0         1   \n",
       "class_14        0         0         2         0         1        26         1   \n",
       "class_15        0         2         0         0         4         0        27   \n",
       "class_16        1         0         0         0         2         1         0   \n",
       "class_17        0         0         1         2         1         0         0   \n",
       "class_18        0         0         0         0         0         1         1   \n",
       "class_19        1         1         0         2         0         0         3   \n",
       "class_2         0         0         0         0         1         1         2   \n",
       "class_20        4         0         0         0         0         2         1   \n",
       "class_3         0         0         1         1         1         0         0   \n",
       "class_4         0         1         0         0         2         0         0   \n",
       "class_5         0         0         0         0         1         0         0   \n",
       "class_6         0         0         0         1         0         0         0   \n",
       "class_7         0         0         0         0         2         0         2   \n",
       "class_8         1         0         0         0         3         0         0   \n",
       "class_9         2         0         1         1         2         1         0   \n",
       "\n",
       "          class_16  class_17  class_18  class_19  class_2  class_20  class_3  \\\n",
       "class_1          0         0         2         0        0        10        0   \n",
       "class_10         0         0         0         0        0         0        2   \n",
       "class_11         0         0         0         1        0         0        0   \n",
       "class_12         0         0         0         2        1         1        0   \n",
       "class_13         0         0         0         0        0         0        1   \n",
       "class_14         1         0         0         0        2         0        2   \n",
       "class_15         0         0         0         1        0         0        0   \n",
       "class_16        18         0         0         1        2         7        0   \n",
       "class_17         1        19         0         2        0         0        0   \n",
       "class_18         0         1        22         4        0         0        1   \n",
       "class_19         3         0         0        13        0         1        0   \n",
       "class_2          2         0         0         0       19         1        4   \n",
       "class_20         6         1         1         2        0        11        0   \n",
       "class_3          0         0         0         0        3         0       24   \n",
       "class_4          0         0         0         0        3         0        3   \n",
       "class_5          0         0         0         0        2         0        2   \n",
       "class_6          0         0         0         0        2         0        4   \n",
       "class_7          0         0         2         0        0         0        0   \n",
       "class_8          1         0         0         0        0         0        3   \n",
       "class_9          0         0         0         1        0         0        1   \n",
       "\n",
       "          class_4  class_5  class_6  class_7  class_8  class_9  \n",
       "class_1         0        0        0        0        0        0  \n",
       "class_10        0        0        0        0        1        1  \n",
       "class_11        0        0        0        0        2        0  \n",
       "class_12        0        2        1        0        0        0  \n",
       "class_13        2        2        2        3        2        1  \n",
       "class_14        1        0        1        1        1        1  \n",
       "class_15        1        0        1        0        0        1  \n",
       "class_16        0        0        0        0        0        0  \n",
       "class_17        0        0        1        1        2        2  \n",
       "class_18        0        0        0        0        1        0  \n",
       "class_19        0        1        0        0        1        0  \n",
       "class_2         4        1        4        1        0        1  \n",
       "class_20        1        0        1        0        0        1  \n",
       "class_3         6        0        3        1        1        0  \n",
       "class_4        16        4        1        1        0        1  \n",
       "class_5         7       17        1        0        1        0  \n",
       "class_6         2        0       21        1        0        0  \n",
       "class_7         2        0        1       24        1        0  \n",
       "class_8         2        0        0        0       21        6  \n",
       "class_9         1        0        0        1        3       24  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val,y_pred_nw1),columns=y_val.sort_values().unique(),index=y_val.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "####for whole data set\n",
    "\n",
    "y_train_enc=tf.keras.utils.to_categorical(le.fit_transform(y.values))\n",
    "\n",
    "y_test_enc=tf.keras.utils.to_categorical(le.transform(ytest.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(len(train_features_w.columns),1))\n",
    "drop20 = layers.SpatialDropout1D(0.3)(input)\n",
    "conv2 = layers.Conv1D(filters=128, kernel_size=5, activation='relu')(drop20)\n",
    "drop21 = layers.Dropout(0.5)(conv2)\n",
    "conv22 = layers.Conv1D(filters=64, kernel_size=5, activation='relu')(drop21)\n",
    "drop22 = layers.Dropout(0.5)(conv22)\n",
    "pool2 = layers.MaxPooling1D(pool_size=2)(drop22)\n",
    "flat2 = layers.Flatten()(pool2)\n",
    "drop22 = layers.Dropout(0.3)(flat2)\n",
    "dense1 = layers.Dense(32)(drop22)\n",
    "#flat2 = layers.Flatten()(pool2)\n",
    "drop22 = layers.Dropout(0.3)(dense1)\n",
    "out = layers.Dense(20, activation='softmax')(drop22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 2948, 1)]         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 2948, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 2944, 128)         768       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 2944, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 2940, 64)          41024     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2940, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 1470, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 94080)             0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 94080)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                3010592   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                660       \n",
      "=================================================================\n",
      "Total params: 3,053,044\n",
      "Trainable params: 3,053,044\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_nw2 = Model(input, out)\n",
    "model_nw2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_nw2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [EarlyStopping(monitor='val_loss', patience=10,min_delta=0.0005),\\\n",
    "                  ModelCheckpoint(filepath='best_model.hdf5', monitor='val_loss',\\\n",
    "                                  save_best_only=True, mode='min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "44/44 [==============================] - 84s 2s/step - loss: 2.6157 - accuracy: 0.2171 - val_loss: 1.9208 - val_accuracy: 0.4759\n",
      "Epoch 2/20\n",
      "44/44 [==============================] - 96s 2s/step - loss: 1.6859 - accuracy: 0.4928 - val_loss: 1.6495 - val_accuracy: 0.5416\n",
      "Epoch 3/20\n",
      "44/44 [==============================] - 82s 2s/step - loss: 1.3666 - accuracy: 0.5809 - val_loss: 1.4641 - val_accuracy: 0.5753\n",
      "Epoch 4/20\n",
      "44/44 [==============================] - 80s 2s/step - loss: 1.1681 - accuracy: 0.6405 - val_loss: 1.4454 - val_accuracy: 0.5763\n",
      "Epoch 5/20\n",
      "44/44 [==============================] - 82s 2s/step - loss: 1.1469 - accuracy: 0.6413 - val_loss: 1.4557 - val_accuracy: 0.5820\n",
      "Epoch 6/20\n",
      "44/44 [==============================] - 81s 2s/step - loss: 1.0345 - accuracy: 0.6770 - val_loss: 1.4881 - val_accuracy: 0.5758\n",
      "Epoch 7/20\n",
      "44/44 [==============================] - 61s 1s/step - loss: 1.0101 - accuracy: 0.6835 - val_loss: 1.4867 - val_accuracy: 0.5872\n",
      "Epoch 8/20\n",
      "44/44 [==============================] - 67s 2s/step - loss: 1.0201 - accuracy: 0.6832 - val_loss: 1.5631 - val_accuracy: 0.5773\n",
      "Epoch 9/20\n",
      "44/44 [==============================] - 59s 1s/step - loss: 1.0174 - accuracy: 0.6738 - val_loss: 1.5569 - val_accuracy: 0.5980\n",
      "Epoch 10/20\n",
      "44/44 [==============================] - 77s 2s/step - loss: 0.9885 - accuracy: 0.6911 - val_loss: 1.6262 - val_accuracy: 0.5841\n",
      "Epoch 11/20\n",
      "44/44 [==============================] - 75s 2s/step - loss: 0.9465 - accuracy: 0.7009 - val_loss: 1.6697 - val_accuracy: 0.5835\n",
      "Epoch 12/20\n",
      "44/44 [==============================] - 84s 2s/step - loss: 0.9242 - accuracy: 0.7063 - val_loss: 1.6918 - val_accuracy: 0.5799\n",
      "Epoch 13/20\n",
      "44/44 [==============================] - 78s 2s/step - loss: 0.9537 - accuracy: 0.6965 - val_loss: 1.7684 - val_accuracy: 0.5722\n",
      "Epoch 14/20\n",
      "44/44 [==============================] - 58s 1s/step - loss: 0.9399 - accuracy: 0.7063 - val_loss: 1.8219 - val_accuracy: 0.5727\n"
     ]
    }
   ],
   "source": [
    "history = model_nw2.fit(train_features_w.values,y_train_enc,\n",
    "                epochs=20,\n",
    "                verbose=1,\n",
    "                validation_data=(test_features_w.values, y_test_enc),\n",
    "                batch_size=64,\n",
    "                \n",
    "                #class_weight={0:1,1:10},\n",
    "                callbacks=callbacks_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=le.inverse_transform(np.argmax(model_nw2.predict(test_features_w),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "fval_nw2 = f1_score(ytest,y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy - F1 score weighted Average is 58.13% for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5738420100594281"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fval_nw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_10</th>\n",
       "      <th>class_11</th>\n",
       "      <th>class_12</th>\n",
       "      <th>class_13</th>\n",
       "      <th>class_14</th>\n",
       "      <th>class_15</th>\n",
       "      <th>class_16</th>\n",
       "      <th>class_17</th>\n",
       "      <th>class_18</th>\n",
       "      <th>class_19</th>\n",
       "      <th>class_2</th>\n",
       "      <th>class_20</th>\n",
       "      <th>class_3</th>\n",
       "      <th>class_4</th>\n",
       "      <th>class_5</th>\n",
       "      <th>class_6</th>\n",
       "      <th>class_7</th>\n",
       "      <th>class_8</th>\n",
       "      <th>class_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_1</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_10</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_11</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_12</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_13</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_14</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_15</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_16</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_18</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_20</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_8</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          class_1  class_10  class_11  class_12  class_13  class_14  class_15  \\\n",
       "class_1        34         0         1         1         1         3         3   \n",
       "class_10        2        67        23         0         2         1         2   \n",
       "class_11        1         6        83         0         0         0         2   \n",
       "class_12        2         1         0        66         2         1         5   \n",
       "class_13        2         0         3         1        34         1         3   \n",
       "class_14        3         1         2         2         3        47         6   \n",
       "class_15        2         0         4         1         1         3        74   \n",
       "class_16        4         0         0         0         1         3         3   \n",
       "class_17        1         1         2         4         0         1         1   \n",
       "class_18        8         1         1         0         1         0         3   \n",
       "class_19        1         0         3         1         2         3         4   \n",
       "class_2         1         1         1         0         4         2         3   \n",
       "class_20        7         0         2         0         2         1         1   \n",
       "class_3         0         1         1         0         3         0         0   \n",
       "class_4         0         2         0         1         8         2         1   \n",
       "class_5         1         0         1         0         4         1         5   \n",
       "class_6         0         2         0         1         3         2         2   \n",
       "class_7         1         1         3         0         1         0         3   \n",
       "class_8         0         2         1         1         3         2         3   \n",
       "class_9         2         3         0         0         0         3         1   \n",
       "\n",
       "          class_16  class_17  class_18  class_19  class_2  class_20  class_3  \\\n",
       "class_1          8         1         2         3        3        21        0   \n",
       "class_10         1         0         0         5        2         2        0   \n",
       "class_11         0         0         0         0        1         1        1   \n",
       "class_12         1         4         0         8        4         1        5   \n",
       "class_13         0         0         1         4        6         1        7   \n",
       "class_14         5         0         1         8        1         2        3   \n",
       "class_15         1         2         0         2        2         2        1   \n",
       "class_16        35         0         0         2        0        45        2   \n",
       "class_17         2        53         2         9        0         5        0   \n",
       "class_18         1         3        63         5        0        13        0   \n",
       "class_19         3        12         5        30        2         8        0   \n",
       "class_2          0         0         1         0       63         1        5   \n",
       "class_20         6         3         1         1        0        44        0   \n",
       "class_3          0         1         0         0       13         0       53   \n",
       "class_4          0         0         0         1        4         0       14   \n",
       "class_5          0         0         0         0        1         0        5   \n",
       "class_6          0         1         1         0       16         0       17   \n",
       "class_7          0         0         0         0        7         0        1   \n",
       "class_8          0         4         0         0        1         2        0   \n",
       "class_9          0         1         0         0        0         0        0   \n",
       "\n",
       "          class_4  class_5  class_6  class_7  class_8  class_9  \n",
       "class_1         0        1        0        2        3        1  \n",
       "class_10        0        0        0        2        4        1  \n",
       "class_11        0        0        1        0        1        0  \n",
       "class_12        2        1        0        2        1        0  \n",
       "class_13        8        6        4        1       10        4  \n",
       "class_14        2        0        1        2        1        1  \n",
       "class_15        0        1        1        1        5        2  \n",
       "class_16        0        0        1        0        1        0  \n",
       "class_17        1        1        1        2        0        1  \n",
       "class_18        2        0        0        0        1        2  \n",
       "class_19        0        2        1        0        1        0  \n",
       "class_2         5        2        3        1        3        2  \n",
       "class_20        0        1        0        0        1        0  \n",
       "class_3        15        5        4        2        1        1  \n",
       "class_4        50       10        5        8        1        0  \n",
       "class_5        14       46        6        0        3        0  \n",
       "class_6         5        4       42        1        1        0  \n",
       "class_7         5        5        0       73        3        0  \n",
       "class_8         1        2        0        1       79        5  \n",
       "class_9         2        0        0        4       13       71  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(ytest,y_pred),columns=ytest.sort_values().unique(),index=ytest.sort_values().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrutijain/Applications/Python/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#from gensim.models import Doc2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, errors = 'ignore', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            \n",
    "            word, *vector = line.split()\n",
    "            \n",
    "            if word in word_index:\n",
    "                \n",
    "                idx = word_index[word]\n",
    "                \n",
    "                try:\n",
    "                    embedding_matrix[idx] = np.array(\n",
    "                        vector, dtype=np.float32)[:embedding_dim]\n",
    "                    \n",
    "                except ValueError:\n",
    "                    continue\n",
    "                    \n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(text_series,max_=250,tokenizer=None):\n",
    "    '''\n",
    "        Creates sequences from text data\n",
    "    '''\n",
    "    \n",
    "    if not tokenizer:\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=5000)\n",
    "        \n",
    "        tokenizer.fit_on_texts(text_series)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return pad_sequences(tokenizer.texts_to_sequences(text_series),\\\n",
    "                         padding='post',maxlen=max_),tokenizer\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_functional_column(X1,X2,X_sub,X_org,vocab_size,embedding_dim,embedding_matrix,maxlen,optimizer,\\\n",
    "              layer_s={'conv':{0:(64,3,3),1:(64,3,2)},'lstm':[(16,0.25)\n",
    "                            ],\n",
    "                               'drop':[0.2,0.2],'dense':[32,16]},svocab=1000,\\\n",
    "                  ovocab=1000,smax=25,omax=10\n",
    "              ):\n",
    "#######################################################################################################    \n",
    "    visible = layers.Input(shape=(X1.shape[1],))\n",
    "    \n",
    "    embed=layers.Embedding(vocab_size+1, embedding_dim, \n",
    "                               weights=[embedding_matrix], \n",
    "                               input_length=maxlen, \n",
    "                               trainable=False)(visible)\n",
    "    \n",
    "    last=[embed]\n",
    "    \n",
    "    for layer in layer_s['conv'].keys():\n",
    "       \n",
    "       last.append(layers.Conv1D(layer_s['conv'][layer][0],layer_s['conv'][layer][1]\\\n",
    "                               ,strides=layer_s['conv'][layer][2],activation='relu',padding='valid')(last[-1]))\n",
    "\n",
    "    \n",
    "    for n_layer in range(len(layer_s['lstm'])):\n",
    "        \n",
    "        if n_layer==len(layer_s['lstm'])-1:\n",
    "            \n",
    "            last.append(layers.LSTM(layer_s['lstm'][n_layer][0], return_sequences=False)(last[-1]))\n",
    "            \n",
    "            last.append(layers.Dropout(layer_s['lstm'][n_layer][1])(last[-1]))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            last.append(layers.LSTM(layer_s['lstm'][n_layer][0], return_sequences=True)(last[-1]))\n",
    "            \n",
    "            last.append(layers.Dropout(layer_s['lstm'][n_layer][1])(last[-1]))\n",
    "            \n",
    "    last.append(layers.BatchNormalization()(last[-1]))\n",
    "    \n",
    "    sub_input= layers.Input(shape=(X_sub.shape[1],))\n",
    "    \n",
    "    embed1=layers.Embedding(svocab+1, embedding_dim, \n",
    "                               weights=[embedding_matrix], \n",
    "                               input_length=25, \n",
    "                               trainable=False)(sub_input)\n",
    "#######################################################################################################    \n",
    "    sub_conv=layers.Conv1D(8,2,strides=1,activation='relu',padding='valid')(embed1)\n",
    "    \n",
    "    lstm=layers.LSTM(8, return_sequences=False)(sub_conv)\n",
    "    \n",
    "    lstm_drp=layers.Dropout(0.2)(lstm)\n",
    "    \n",
    "    lstm_drp=layers.BatchNormalization()(lstm_drp)\n",
    "    \n",
    "    sub0 = layers.Dense(16, activation='relu')(lstm_drp)\n",
    "    \n",
    "    sub0b=layers.BatchNormalization()(sub0)\n",
    "    \n",
    "    sub0b=layers.Dropout(0.1)(sub0b)\n",
    "    \n",
    " #######################################################################################################  \n",
    "    org_input= layers.Input(shape=(X_org.shape[1],))\n",
    "    \n",
    "    embed1=layers.Embedding(ovocab+1, embedding_dim, \n",
    "                               weights=[embedding_matrix], \n",
    "                               input_length=10, \n",
    "                               trainable=False)(org_input)\n",
    "    \n",
    "    org_conv=layers.Conv1D(8,2,strides=1,activation='relu',padding='valid')(embed1)\n",
    "    \n",
    "    lstm=layers.LSTM(8, return_sequences=False)(org_conv)\n",
    "    \n",
    "    lstm_drp=layers.Dropout(0.2)(lstm)\n",
    "    \n",
    "    lstm_drp=layers.BatchNormalization()(lstm_drp)\n",
    "    \n",
    "    org0 = layers.Dense(16, activation='relu')(lstm_drp)\n",
    "    \n",
    "    org0b=layers.BatchNormalization()(org0)\n",
    "    \n",
    "    org0b=layers.Dropout(0.1)(org0b)\n",
    "    \n",
    " #######################################################################################################   \n",
    "            \n",
    "    other_input = layers.Input(shape=(X2.shape[1], ))\n",
    "    \n",
    "    hidden0 = layers.Dense(16, activation='relu')(other_input)\n",
    "    \n",
    "    hidden0b=layers.BatchNormalization()(hidden0)\n",
    "    \n",
    "    hidden0b=layers.Dropout(0.1)(hidden0b)\n",
    "    \n",
    "    hidden1 = layers.Dense(16, activation='relu')(hidden0b)\n",
    "    \n",
    "    hidden1b=layers.BatchNormalization()(hidden1)\n",
    "\n",
    "#######################################################################################################\n",
    "    \n",
    "    merge = layers.Concatenate()([last[-1], hidden1b, org0b, sub0b])\n",
    "    \n",
    "    hidden2 = layers.Dense(128, activation='relu')(merge)\n",
    "    \n",
    "    hidden2b=layers.BatchNormalization()(hidden2)\n",
    "    \n",
    "    hidden2b=layers.Dropout(0.2)(hidden2b)\n",
    "    \n",
    "    hidden3 = layers.Dense(64, activation='relu')(hidden2b)\n",
    "    \n",
    "    output=layers.Dense(20, activation='softmax')(hidden3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[visible,other_input,sub_input,org_input], outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  #loss=Custom_Loss(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "#summarize layers\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(vocab_size,embedding_dim,embedding_matrix,maxlen,optimizer,\\\n",
    "              layer_s={'conv':{0:(64,3),1:(64,3)},'lstm':[(64,0.25)\n",
    "                                    ,(32,0.25)\n",
    "                            ],\n",
    "                               'drop':[0.5,0.5],'dense':[32,16]},\n",
    "              ):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Embedding(vocab_size+1, embedding_dim, \n",
    "                               weights=[embedding_matrix], \n",
    "                               input_length=maxlen, \n",
    "                               trainable=False))\n",
    "    \n",
    "    for layer in layer_s['conv'].keys():\n",
    "        \n",
    "        model.add(layers.Conv1D(layer_s['conv'][layer][0],layer_s['conv'][layer][1]\\\n",
    "                                ,strides=layer_s['conv'][layer][2],activation='relu',padding='valid'))\n",
    "            \n",
    "    \n",
    "    \n",
    "        \n",
    "    #model.add(layers.BatchNormalization())\n",
    "        \n",
    "    \n",
    "    #model.add(layers.Dropout(layer_s['drop'][0]))\n",
    "        \n",
    "    \n",
    "    for n_layer in range(len(layer_s['lstm'])):\n",
    "        \n",
    "        if n_layer==len(layer_s['lstm'])-1:\n",
    "            \n",
    "            model.add(layers.LSTM(layer_s['lstm'][n_layer][0], return_sequences=False))\n",
    "            \n",
    "            model.add(layers.Dropout(layer_s['lstm'][n_layer][1]))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            model.add(layers.LSTM(layer_s['lstm'][n_layer][0], return_sequences=True))\n",
    "            \n",
    "            model.add(layers.Dropout(layer_s['lstm'][n_layer][1]))\n",
    "            \n",
    "        \n",
    "    model.add(layers.BatchNormalization())\n",
    "        \n",
    "        \n",
    "    for n_ in range(len(layer_s['dense'])):\n",
    "        \n",
    "        model.add(layers.Dense(layer_s['dense'][n_],activation='relu'))\n",
    "        \n",
    "        model.add(layers.Dropout(layer_s['drop'][n_]))\n",
    "        \n",
    "    \n",
    "    model.add(layers.Dense(20, activation='softmax')) # for twenty y classes\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  #loss=Custom_Loss(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLoss = 1000000000000000000\n",
    "bestWeights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(lr=0.05)\n",
    "\n",
    "embedding_dim=100\n",
    "\n",
    "maxlen=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence,tkn = create_sequences(X_train.other_text.values,max_=maxlen)\n",
    "\n",
    "test_sequence,tkn = create_sequences(X_val.other_text.values,tokenizer=tkn,max_=maxlen)\n",
    "\n",
    "vocab_size=len(tkn.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence_sub,stkn = create_sequences(X_train.subject.values,max_=25)\n",
    "\n",
    "test_sequence_sub,stkn = create_sequences(X_val.subject,tokenizer=tkn,max_=25)\n",
    "\n",
    "svocab_size=len(stkn.word_index)\n",
    "\n",
    "\n",
    "train_sequence_org,otkn = create_sequences(X_train.organization.values,max_=10)\n",
    "\n",
    "test_sequence_org,otkn = create_sequences(X_val.organization.values,tokenizer=tkn,max_=10)\n",
    "\n",
    "ovocab_size=len(stkn.word_index)\n",
    "\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('/Users/shrutijain/Downloads/glove.6B.100d.txt',tkn.word_index, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 10, 100)      4534800     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 25, 100)      4534800     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 9, 8)         1608        embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 24, 8)        1608        embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 500, 100)     4534800     input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 2295)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 8)            544         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 8)            544         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 166, 64)      19264       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 16)           36736       input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 8)            0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 8)            0           lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 82, 64)       12352       conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16)           64          dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8)            32          dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8)            32          dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 16)           5184        conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 16)           0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 16)           144         batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 16)           144         batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 16)           0           lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 16)           272         dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16)           64          dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16)           64          dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16)           64          dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16)           64          dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 16)           0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 16)           0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64)           0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "                                                                 dropout_38[0][0]                 \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 128)          8320        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128)          512         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 128)          0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 64)           8256        dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 20)           1300        dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 13,701,572\n",
      "Trainable params: 96,724\n",
      "Non-trainable params: 13,604,848\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=get_functional_column(train_sequence,train_features,\\\n",
    "                            train_sequence_sub,train_sequence_org,\\\n",
    "                            vocab_size,embedding_dim,embedding_matrix,maxlen,optimizer,\\\n",
    "                            svocab=svocab_size,ovocab=ovocab_size,smax=25,omax=10\n",
    "          )\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "callbacks_list = [EarlyStopping(monitor='val_loss', patience=10,min_delta=0.0005),\\\n",
    "                  ModelCheckpoint(filepath='best_model.hdf5', monitor='val_loss',\\\n",
    "                                  save_best_only=True, mode='min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 4s 254ms/step - loss: 3.6403 - accuracy: 0.0568 - val_loss: 212.0939 - val_accuracy: 0.0621\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 3.0628 - accuracy: 0.0597 - val_loss: 10.7983 - val_accuracy: 0.0607\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 3s 173ms/step - loss: 3.0197 - accuracy: 0.0530 - val_loss: 8.9520 - val_accuracy: 0.0564\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 3.0103 - accuracy: 0.0491 - val_loss: 3.0796 - val_accuracy: 0.0578\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 3s 173ms/step - loss: 3.0023 - accuracy: 0.0564 - val_loss: 3.0239 - val_accuracy: 0.0592\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 3s 158ms/step - loss: 3.0013 - accuracy: 0.0607 - val_loss: 3.0259 - val_accuracy: 0.0592\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 2.9984 - accuracy: 0.0539 - val_loss: 2.9959 - val_accuracy: 0.0578\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 3s 173ms/step - loss: 2.9996 - accuracy: 0.0544 - val_loss: 2.9904 - val_accuracy: 0.0592\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 3s 166ms/step - loss: 2.9985 - accuracy: 0.0597 - val_loss: 2.9910 - val_accuracy: 0.0592\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 2.9987 - accuracy: 0.0602 - val_loss: 2.9902 - val_accuracy: 0.0592\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 2.9987 - accuracy: 0.0496 - val_loss: 2.9904 - val_accuracy: 0.0592\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 2.9934 - accuracy: 0.0621 - val_loss: 2.9909 - val_accuracy: 0.0592\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 4s 236ms/step - loss: 3.0005 - accuracy: 0.0583 - val_loss: 2.9896 - val_accuracy: 0.0592\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 3s 190ms/step - loss: 2.9976 - accuracy: 0.0564 - val_loss: 2.9905 - val_accuracy: 0.0592\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 2.9981 - accuracy: 0.0539 - val_loss: 2.9904 - val_accuracy: 0.0592\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 2.9951 - accuracy: 0.0588 - val_loss: 2.9900 - val_accuracy: 0.0592\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 3s 186ms/step - loss: 2.9995 - accuracy: 0.0544 - val_loss: 2.9901 - val_accuracy: 0.0592\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 3s 194ms/step - loss: 2.9997 - accuracy: 0.0592 - val_loss: 2.9908 - val_accuracy: 0.0592\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 3s 159ms/step - loss: 2.9987 - accuracy: 0.0549 - val_loss: 2.9907 - val_accuracy: 0.0592\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 3s 186ms/step - loss: 2.9975 - accuracy: 0.0530 - val_loss: 2.9911 - val_accuracy: 0.0592\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 2.9979 - accuracy: 0.0515 - val_loss: 2.9902 - val_accuracy: 0.0592\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 3s 177ms/step - loss: 2.9957 - accuracy: 0.0568 - val_loss: 2.9902 - val_accuracy: 0.0592\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 2.9958 - accuracy: 0.0597 - val_loss: 2.9905 - val_accuracy: 0.0592\n"
     ]
    }
   ],
   "source": [
    "le=LabelEncoder()    \n",
    "    \n",
    "y_train_enc=tf.keras.utils.to_categorical(le.fit_transform(y_train.values))\n",
    "\n",
    "y_test_enc=tf.keras.utils.to_categorical(le.transform(y_val.values))\n",
    "\n",
    "#%%\n",
    "X_train_new=[train_sequence,train_features,\\\n",
    "                            train_sequence_sub,train_sequence_org]\n",
    "    \n",
    "X_test_new=[test_sequence,test_features,\\\n",
    "                            test_sequence_sub,test_sequence_org]\n",
    "    \n",
    "    \n",
    "history = model.fit(X_train_new,y_train_enc,\n",
    "                epochs=50,\n",
    "                verbose=1,\n",
    "                validation_data=(X_test_new, y_test_enc),\n",
    "                batch_size=128,\n",
    "                \n",
    "                #class_weight={0:1,1:10},\n",
    "                callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
